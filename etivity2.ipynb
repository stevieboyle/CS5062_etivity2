{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold #needed for multidimensional scaling (MDS) and t-SNE\n",
    "from sklearn import cluster #needed for k-Means clustering\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, FunctionTransformer #needed for data preparation\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(number of examples, number of attributes):  (2000, 17)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./bank.csv\")\n",
    "print('(number of examples, number of attributes): ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import manifold #needed for multidimensional scaling (MDS) and t-SNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"bank.csv\")\n",
    "\n",
    "colors = np.array(['orange', 'blue', 'lime', 'khaki', 'pink', 'green', 'purple'])\n",
    "\n",
    "# points - a 2D array of (x,y) coordinates of data points\n",
    "# labels - an array of numeric labels in the interval [0..k-1], one for each point\n",
    "# centers - a 2D array of (x, y) coordinates of cluster centers\n",
    "# title - title of the plot\n",
    "\n",
    "\n",
    "def clustering_scatterplot(points, labels, centers, title):\n",
    "    \n",
    "    \n",
    "    n_clusters = np.unique(labels).size\n",
    "    for i in range(n_clusters):\n",
    "        h = plt.scatter(points[labels==i,0],\n",
    "                        points[labels==i,1], \n",
    "                        c=colors[i%colors.size],\n",
    "                        label = 'cluster '+str(i))\n",
    "\n",
    "    # plot the centers of the clusters\n",
    "    if centers is not None:\n",
    "        plt.scatter(centers[:,0], centers[:,1], c='r', marker='*', s=500)\n",
    "\n",
    "    _ = plt.title(title)\n",
    "    _ = plt.legend()\n",
    "    _ = plt.xlabel('x')\n",
    "    _ = plt.ylabel('y')\n",
    "\n",
    "# Map month abbreviations to numeric months\n",
    "month_map = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "\n",
    "def parse_date_to_weekday(date_str):\n",
    "    # Split by underscore -> ['17', 'feb']\n",
    "    day_str, month_str = date_str.split('_')\n",
    "    day = int(day_str)\n",
    "    month = month_map[month_str]\n",
    "    \n",
    "    # Use an arbitrary year, e.g., 2023\n",
    "    dt = datetime(2022, month, day)\n",
    "    # weekday(): Monday=0, Sunday=6; or use strftime('%A') for the name\n",
    "    return dt.weekday()\n",
    "\n",
    "\n",
    "\n",
    "# combine day and month columns into a single column, skip any missing values\n",
    "df['contact_date'] = df['day'].astype(str) + '_' + df['month']\n",
    "\n",
    "# extract contact date as a string array.  Apply the function to each date\n",
    "df['contact_weekday'] = df['contact_date'].apply(parse_date_to_weekday)\n",
    "\n",
    "# drop the original contact date column\n",
    "df.drop(columns=['contact_date'], inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "df_X = df.drop(columns=[\"subscribed\"])\n",
    "df_Y = df[\"subscribed\"]\n",
    "\n",
    "# fill missing values\n",
    "df_X['age'] = df_X['age'].fillna(df_X['age'].median())\n",
    "\n",
    "# fill the missing job values with a new category called 'unknown'\n",
    "df_X['job'] = df_X['job'].fillna('unknown')\n",
    "\n",
    "# fill the missing poutcome values with a new category called 'unknown'\n",
    "df_X['poutcome'] = df_X['poutcome'].fillna('unknown')\n",
    "\n",
    "# fill the missing contact values with a new category called 'unknown'\n",
    "df_X['contact'] = df_X['contact'].fillna('unknown')\n",
    "\n",
    "# fill the missing education values with a new category called 'unknown'\n",
    "df_X['education'] = df_X['education'].fillna('unknown')\n",
    "\n",
    "# Encode categorical features\n",
    "\n",
    "# job\n",
    "job_dummies = pd.get_dummies(df_X['job'], prefix='job')\n",
    "df_X = pd.concat([df_X, job_dummies], axis=1)\n",
    "df_X.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# marital\n",
    "marital_dummies = pd.get_dummies(df_X['marital'], prefix='marital')\n",
    "df_X = pd.concat([df_X, marital_dummies], axis=1)\n",
    "df_X.drop('marital', axis=1, inplace=True)\n",
    "\n",
    "# contact\n",
    "contact_dummies = pd.get_dummies(df_X['contact'], prefix='contact')\n",
    "df_X = pd.concat([df_X, contact_dummies], axis=1)\n",
    "df_X.drop('contact', axis=1, inplace=True)\n",
    "\n",
    "# poutcome\n",
    "poutcome_dummies = pd.get_dummies(df_X['poutcome'], prefix='poutcome')\n",
    "df_X = pd.concat([df_X, poutcome_dummies], axis=1)\n",
    "df_X.drop('poutcome', axis=1, inplace=True)\n",
    "\n",
    "# default\n",
    "df_X['default'] = df_X['default'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# housing\n",
    "df_X['housing'] = df_X['housing'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# loan\n",
    "df_X['loan'] = df_X['loan'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "month_order = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "df_X['month'] = df_X['month'].map(month_order)\n",
    "\n",
    "# add sin and cosine transformation for month\n",
    "df_X['month_sin'] = np.sin(2 * np.pi * df_X['month'] / 12)          \n",
    "df_X['month_cos'] = np.cos(2 * np.pi * df_X['month'] / 12)\n",
    "\n",
    "edu_order = {\n",
    "    'unknown': -1,\n",
    "    'primary': 1,\n",
    "    'secondary': 2,\n",
    "    'tertiary': 3\n",
    "}\n",
    "\n",
    "df_X['education_level'] = df_X['education'].map(edu_order)\n",
    "df_X['education_unknown'] = (df_X['education'] == 'unknown').astype(int)\n",
    "\n",
    "# Drop original column if desired\n",
    "df_X.drop('education', axis=1, inplace=True)\n",
    "\n",
    "df_X['day_sin'] = np.sin(2 * np.pi * df_X['day'] / max(df_X['day']))\n",
    "\n",
    "# add sin and cos for contact weekday\n",
    "df_X['contact_weekday_sin'] = np.sin(2 * np.pi * df_X['contact_weekday'] / 7)\n",
    "df_X['contact_weekday_cos'] = np.cos(2 * np.pi * df_X['contact_weekday'] / 7)\n",
    "\n",
    "# handle outliers\n",
    "\n",
    "# age use IQR clipping\n",
    "Q1 = df_X['age'].quantile(0.25)\n",
    "Q3 = df_X['age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df_X['age'] = np.clip(df_X['age'], lower_bound, upper_bound)\n",
    "\n",
    "# balance use log transform\n",
    "df_X['balance'] = np.log(df_X['balance'] + abs(df_X['balance'].min()) + 1)\n",
    "\n",
    "# duration use log transform\n",
    "df_X['duration'] = np.log(df_X['duration'] + abs(df_X['duration'].min()) + 1)\n",
    "\n",
    "# campaign use IQR clipping\n",
    "Q1 = df_X['campaign'].quantile(0.25)\n",
    "Q3 = df_X['campaign'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df_X['campaign'] = np.clip(df_X['campaign'], lower_bound, upper_bound)\n",
    "\n",
    "# previous use log transform\n",
    "df_X['previous'] = np.log(df_X['previous'] + abs(df_X['previous'].min()) + 1)\n",
    "\n",
    "# handle Skewed scaling\n",
    "\n",
    "# age use StandardScaler\n",
    "df_X['age'] = StandardScaler().fit_transform(df_X[['age']])\n",
    "\n",
    "# balance use standardScaler\n",
    "df_X['balance'] = StandardScaler().fit_transform(df_X[['balance']])\n",
    "\n",
    "# day minmax scaler\n",
    "df_X['day'] = MinMaxScaler().fit_transform(df_X[['day']])\n",
    "\n",
    "# duration use standardScaler\n",
    "df_X['duration'] = StandardScaler().fit_transform(df_X[['duration']])\n",
    "\n",
    "# campaign use robustScaler\n",
    "df_X['campaign'] = RobustScaler().fit_transform(df_X[['campaign']])\n",
    "\n",
    "# pdays use standardScaler\n",
    "df_X['pdays'] = StandardScaler().fit_transform(df_X[['pdays']])\n",
    "\n",
    "# previous use standardScaler\n",
    "df_X['previous'] = StandardScaler().fit_transform(df_X[['previous']])\n",
    "\n",
    "print(df_X.head())\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `preprocessed_df` is your manually preprocessed DataFrame\n",
    "k = 3  # Number of clusters\n",
    "\n",
    "# Initialize the KMeans model\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# Fit the model to the preprocessed data\n",
    "kmeans.fit(df_X)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Get the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# extract the processed data as a numpy array\n",
    "preprocessed_data = df_X.to_numpy()\n",
    "# combine the preprocessed data and cluster centers\n",
    "data_and_centers = np.r_[preprocessed_data, cluster_centers]\n",
    "# apply MDS to the combined data\n",
    "XYcoordinates = manifold.MDS(n_components=2, normalized_stress='auto').fit_transform(data_and_centers)\n",
    "print(\"Transformation complete\")\n",
    "\n",
    "# Determine the number of points in XYcoordinates.\n",
    "n_points = XYcoordinates.shape[0]\n",
    "\n",
    "# use clustering scatterplot function to visualize the clusters\n",
    "clustering_scatterplot(points=XYcoordinates[:-k, :], \n",
    "                       labels=cluster_labels[:n_points-k], \n",
    "                       centers=XYcoordinates[-k:, :], \n",
    "                       title=\"KMeans Clustering with MDS\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# add the cluster labels to the original DataFrame\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# pivot the subscribed column to see the distribution of subscription status in each cluster\n",
    "df_pivot = df.pivot_table(index='cluster', columns='subscribed', aggfunc='size', fill_value=0)\n",
    "df_pivot = df_pivot.reset_index()\n",
    "df_pivot.columns.name = None  # Remove the name of the columns index\n",
    "df_pivot.columns = ['cluster', 'no', 'yes']  # Rename the columns\n",
    "df_pivot['total'] = df_pivot['no'] + df_pivot['yes']\n",
    "df_pivot['yes_percentage'] = df_pivot['yes'] / df_pivot['total'] * 100\n",
    "df_pivot['no_percentage'] = df_pivot['no'] / df_pivot['total'] * 100\n",
    "df_pivot = df_pivot.sort_values(by='yes_percentage', ascending=False)\n",
    "df_pivot.reset_index(drop=True, inplace=True)\n",
    "print(df_pivot)\n",
    "\n",
    "# Replace the problematic sections with this code\n",
    "\n",
    "from sklearn.manifold import TSNE  # Import TSNE\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Combine the preprocessed data and cluster centers before applying t-SNE\n",
    "combined_data = np.vstack([preprocessed_data, cluster_centers])\n",
    "\n",
    "# Apply t-SNE to the combined data\n",
    "tsne_results_combined = tsne.fit_transform(combined_data)\n",
    "\n",
    "# Split the results back into data points and centers\n",
    "tsne_data_points = tsne_results_combined[:-k]\n",
    "tsne_centers = tsne_results_combined[-k:]\n",
    "\n",
    "# No need for data_and_centers_tsne - just use the results directly\n",
    "print(\"t-SNE transformation complete\")\n",
    "\n",
    "# Use clustering scatterplot function to visualize the clusters\n",
    "clustering_scatterplot(points=tsne_data_points, \n",
    "                      labels=cluster_labels, \n",
    "                      centers=tsne_centers, \n",
    "                      title=\"KMeans Clustering with t-SNE\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# apply elbow method to find the optimal number of clusters\n",
    "def elbow_method(data, max_k):\n",
    "    sse = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, max_k + 1), sse, marker='o')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the elbow method function\n",
    "elbow_method(preprocessed_data, max_k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
